{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview\n",
    "This notebook was created to summarize the main ideas from the 1st chapter of the Information Retrieval Book: [chap1](https://nlp.stanford.edu/IR-book/pdf/01bool.pdf)\n",
    "\n",
    "## Terminology\n",
    "* The term ***'document'*** refers to whatever block / unit we use to build the ***IR*** system, to extract information from. The group of documents is referred to as ***'collection'***\n",
    "* ***'term'*** refers to an unreductible unit of text: mainly a token in NLP jargon. (not necessarily a word)\n",
    "* Precision: what fraction of the returned results are relevant to the user (as you can see the term is different than in the Machine Learning literature)\n",
    "* recall: what fraction of the relevant results were returned.\n",
    "* term-document matrix: a matrix where the rows are the unique terms and the columns represents documents and the cell is a boolean value representing whether the term is present in the document or not. The meaning of document-term matrix can be inferred accordingly."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inverted Index\n",
    "### Issue with naive term-document matrix\n",
    "let's assume we have $N = 10 ^ 6$ docs, $K = 0.5 \\cdot 10 ^ 6$ unique terms. Out matrix has already more than $10 ^ {11}$ cells. Assuming a maximum length of $10 ^ 4$ words for each document. We can estimate a theoretical bound for the number of non-zero cells in the matrix. Assuming every document is of maximum length, as well as have no repeating terms. Each column will have $10 ^ 4$ 1's out of the $K rows$: $\\frac{10 ^ 4} { 0.5 \\cdot 10 ^ 6} = 2 \\%$. SO regardless of the maximum length of the document, the matrix is significantly sparse. \n",
    "### Inverted Index: construction\n",
    "For efficient retrieval and manipulation, we need to first build out index in advance, iterate through each and every document\n",
    "1. apply different text processing techniques: convert the document to a list of terms (tokens)\n",
    "2. For each of these found tokens, add the document Id to the corresponding dictionary\n",
    "3. The terms should be sorted, and the document frequency of a word should be saved as well with the ***'term's*** frequency.\n",
    "4. Each term now maps to a list (usually stored in disk) of documents IDs.\n",
    "\n",
    "Given a term, we now have access to the documents, where the term in question appears"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
