{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview\n",
    "This notebook was created to save notes of the Information retrieval course offered by Innopolis University."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week2: Web Basics\n",
    "* the internet protocol is a mechanism to assign a $4$ bytes to a machine connected to the Internet. This address is crucial to transfer data from servers to clients (which are generally browsers). Certain IP addresses are reserved for specific use-cases such as 127.0.0.1 : which refers the machine to itself.  \n",
    "* One false impression about IP addresses is that they are static. but they aren't. a certain organization is taking care of organizing the distribuation of IP addresses. The same organization announced the birth of a new era of the Internet Protocol as all the previous 4-bytes address were already occupied. \n",
    "* One important note: IP $\\neq$ machine. It took me a while to see this.  A router masks the IP address of the local network. In other words, any request generated by a local computer is associated with the router's IP address. So to the outside world, each and every computer have the same IP address.\n",
    "* the distribution of IP addresses is hierarchical. The associated sells ranges of IP address to certain Telecommunication companies. Those distribute the ranges even further accross regions or countries. Thus, a certain IP address is tightly associated with the geographical location: the efficiency: 99% country-wise, 95% city-wise.\n",
    "* URI: unique resource identifie: machine readable text following a certain set of rules to uniquely identify a resource on the web. This is an abstract tool as there are two subsets:\n",
    "1. URL: UR locator, URN: UR name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What website would you like to check ??\n",
      "217.69.139.200\n"
     ]
    }
   ],
   "source": [
    "import socket\n",
    "\n",
    "s = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n",
    "\n",
    "print(\"What website would you like to check ??\")\n",
    "\n",
    "site = input()\n",
    "\n",
    "print(socket.gethostbyname(site))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* a good point to keep in mind is IP addresses are in the majority of cases not static. For example, the Internet Service Providers (generally abbreviated to ISPs) generally acquire a set of shared IP addresses. They are distributed between the users dynamically. Such process is definitely more cost-efficient. Websites and companies cannot afford to work with dynamic IP addresses as it might intervene with their DNS queries. \n",
    "* The principal is the same also for private networks. A router has a public address and a translation \n",
    "* Depending on the network the computer is connected to, certain sites can be blocked: mainly depending on whether the ISP is bound to certain national and legal regulations\n",
    "* Domain Name System introduces a hierarichal structure to the internet. DNS protocol maps each human-readable domain name to a certain ip address depending on several factors such as the request's source\n",
    "* several sites (domain names) can have the same IP address for several reasons: a site is not hosted on a didicated physical machine but on a virtual machine installed on physical machine. The latter can host multiple VMs and thus multiple sites which all end up having the same IP address. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "let's consolidate the understanding by answering and solving a number of questions: \n",
    "1. In the office, one website opens up to me but another does not. The behavior is similar for my colleague. why ?\n",
    "    * The ISP for the the office is filtering the second website (network related)\n",
    "    * The machine/server responsible for the 2nd site is simply shut down (the site is down regardless of the client)\n",
    "    * generally any problem in the chain of requests / sites\n",
    "2. if a site opens up to a friend but does not open up to you. The issues probably resides in the local set up\n",
    "3. Certain sites might block a crawler when they notice the huge amount of traffic coming from that particular machine\n",
    "\n",
    "* Cookies overcome the stateless nature of HTML\n",
    "* The evolution of the web:\n",
    "    1. web 1.0: HTML: subset of SGML, SGML parsers, static websites\n",
    "    2. web 2.0: XHTML: HTML + XML , XML parsers, service-oriented network   \n",
    "    3. web 3.0: [x]HMTL5: HTML/XML parsers, content-oriented network: semantically reach documents\n",
    "* The strictness of HTML made the latter readable by machines."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* URI: stands for Unique Resource Identifier. It is of the general format: schema.schema-specific-part. It is abstraction and the concrete implementations of this identifier are\n",
    "    1. URL: URlocator describes the location of the resource: schema how to behave which protocol mechanism to use. location: where to go\n",
    "    2. URN: URname uniquely identifies the resource but without specifying its location.\n",
    "* a port is an additional component in the network hierarchy enabling a single machine to respond to multiple clients. Certain ports are generally linked to certain protocols, http with the port number 80 for example.\n",
    "* how does an url work:   \n",
    "<center>protocol://login:password(optional).domain_name:port(optional)/Path_to_resource/additional_identifier\n",
    "</center>\n",
    "\n",
    "* A little bit more about HTTP: \n",
    "\n",
    "1. Head request: getting meta data of a HTML page: for internal technical interactions, might be used to optimize the GET request further\n",
    "2. The Post Request was further broken down into more specific uploading functionalities: PUT, PATCH, ...  \n",
    "3. starting from http v1.1, HTTP supports sending multiple requests though a single TCP connection.\n",
    "4. HTTP2 introduces multiplexing over a single TCP connection."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* assuming two different domain names are hosted on the same physical machine and they are associated with the same IP addresses, then when establishing a TCP connection to that physical machine, the post part of the HTTP request can determine which of those sites (hosted on the same physical machine) are addressed.\n",
    "* The requests can be categorized depending on their relation to states:\n",
    "\n",
    "1. safe: do not change anything in the server\n",
    "2. non-safe: change the server's state:\n",
    "    * idempotent: F(state) == F(F(state)), the server's state changes solely in the first time even when the request is sent multiple times\n",
    "    * non-idempotent: each single instance of this type of requests change the server."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## web Security\n",
    "* identification: associating an entity to an object that uniquely identifies that entity.\n",
    "* authentication: verifying the match between the entity and its identifier.\n",
    "* authorization: granting access to certain resources for certain identities.\n",
    "* Using the HTTP authentification mechanisms is quite unsecure as it is plain data and can be easily extracted. Even though the data can be encrypted, the HTML documents can be saved and the encrypted data can be converted to its original format with enough time.\n",
    "* Using the OAuth mechanisms, overcomes these security threaths while enhancing the user's experience by enabling them to connect to the service through different popular softwares. We can choose to login through Google, Facebook, Instagram to multiple sites. So the user authenticates to the 3rd-party software, the service, authentication server maps the 3rd-party account to the user's account in the service by issuing an access token. Access to data is offered to the client / 3rd party account by checking the access token.\n",
    "\n",
    "* Decoupling the authentification and the data extraction processes enable the service to protect the data even further by banning the suspecious softwares from all clients without affecting the individual physical clients  "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* These two options, despite the differences in the extent of their functionalities, are based on the same principle: Using HTTP(s) headers.\n",
    "* Modern sites integrated the authentification process as part of the content and the user -experience which is mainly manifested by HTML forums."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 4\n",
    "## Quality Assessment\n",
    "How would a search engine service be assessed:\n",
    "1. The profit generated by the company in question\n",
    "2. The traffic share (once again a moneraty goal)\n",
    "3. user's satisfaction: might be challenging to numerically express such emotional reactions.\n",
    "4. Logins & subscriptions: Logins enable the company to provide personalized services better suited to the user's profile (ethnicity, region, age, financial situation)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Browsers uses Javascript snippets to identify which sites the users browse, as well as facilitate advertisements\n",
    "* Models (ML or even statistical ones) based on complex human bahavior can be challenging. As such human behavior cannot be replicated introducing several possible issues (the difference between train and test data...) \n",
    "* The abstract mainly emotional human metrics require complex statistical formulas.\n",
    "* Evaluation can be categorized mainly into:\n",
    "    1. online: usually involves users in the processes by having two different groups utilize two versions of the services. The main tool is A/B testing. The interpretation and reliability of this approach is quite tied to the time frame used in comparison. The shorter the timefram is, the closer correlation is to causation.\n",
    "    2. offline: executed on a representative sample of the population of the service's users. Done by assessors. The agreement between different assessors in crucial.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What metrics can we use to assess relevance: \n",
    "1. accuracy: does not seem to be possible as the number of possible results can be extremely huge, the queries vary a lot\n",
    "2. precision: how many relevant documents out those retrieved\n",
    "3. recall: how many relevant retrived how of relevant.\n",
    "4. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week: 5 text Search, Indexing\n",
    "Before delving into search and indexing techniques, it might be useful to consider the different types of languages:\n",
    "1. Recursively enumerable: The most free languages: any combination of characters inside the alphabet produces a valid expression\n",
    "2. Context-sensitive : Turing matchine\n",
    "3. Context-free: Push Down Automate\n",
    "4. Regular: A -> a or A -> aB | A -> Ba: going from a non-terminal to a terminal / or either (terminal + non-terminal) / (non-terminal + terminal): finite State Automata\n",
    "\n",
    "* Note to keep in mind: \n",
    "1. stemming reduces the word to whatever is left from the original form of the word   \n",
    "2. lemmatization: convert the word to its root.\n",
    "\n",
    "## Indexing\n",
    "* The most basic way is ***Boolean Retrieval***. Where the query is converted to a boolean function in terms of its tokens: \n",
    "$$ Query = F(f_1(t1), f_2(t2), f_3(t3)... f_k(t_k))$$\n",
    "Where $f_i$ is a boolean operator applied on the $i-th$ token and $F$ is a combination of all these resulting functions.\n",
    "* ONe possible approch is to first find the set of documents that $s_i$ that satisfy the $i-th$ boolean operator and then apply the set operations on these documents to represent the final query result. Such approach is quite expensive: it is at least linear in terms of the number of available documents.\n",
    "* The improvement generally lies in breaking the process into 2 steps: \n",
    "1. in caching / indexing certain documents: preselection phase\n",
    "2. ranking and selecting the result out of this preselected set of documents : applying a more sophisticated ranking criteria on a subset is possible on a smaller set of docs\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Inverted indexing introduces several important optimizations:  \n",
    "In a preprocessing step (not in production):\n",
    "* iterate through all the documents, tokenize, preprocess and save all the resulting tokens into a vocabulary or a lexicon.\n",
    "* Each word in the lexicon now refers to a list populated by pointers to the documents that contains that word: **Posting list**\n",
    "* The **Posting Lists** are generally stored in a sparse matrix structure (instead of using the hardrive which is quite slow!!) "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Building the lexicon /  vocabulary generally comes with a number of issues:\n",
    "1. careful selection of stop words\n",
    "2. more sophisticated tokenization techniques: Pair Byte Encoding for example\n",
    "3. Statistical techniques: N-grams mainly.\n",
    "\n",
    "* Regardless of the index used underneath, the boolean retrieval model is quite inefficient for several reasons:\n",
    "1. The terms used in the query affect to large extent the retrieval results: \n",
    "    * if the term are too specific, then the query will probably under-perform\n",
    "    * if the terms are too general, then the query might not be able to understand the user's actualy need\n",
    "2. The Boolean retrieval uses the Bag of Words which ignores too important factors: the order and the frequency of the terms\n",
    "3. Boolean retrieval is a hard metric, Relevance is soft as it is more of a degree metric."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Ranking is usually associated with the notion of Probability. Items with the highest probability to be relevant should simply be ranked higher. The main point here is that shit is real.\n",
    "* EXTRA READING: CHAP 4"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Zipf's Law: the top $18\\%$ of words account for $80\\%$ of word occurrences. More precisefly:\n",
    "Assuming the most frequent word appears $A$ times, then the word with rank $r$ appears $\\frac{A}{r}$ times."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lecture 6: Spelling checking, query correction \n",
    "So yeah before using the query, a bit more realism: Users would not mind making spelling / grammatical mistakes that the IR system should take into consideration. One possible approach is introducing wild cards:\n",
    "1. $*$ and $?$ representing different possible characters.  \n",
    "2. Solving a more complex problem generally requires introducing more complexity to our program.  \n",
    "\n",
    "The B-Tree is used as an index for the problem at hand, as:\n",
    "1. $far*$ can be solved by using a B-tree index\n",
    "2. $*far$ can be solved using an inversted B-tree index.\n",
    "\n",
    "There are different approaches to solve this issue:\n",
    "1. Pertum index: saving all the possible permutation of a work in the lexicon. The latter enables easier and more efficient retrieval. Nevertheless, this approach still cannot handly multiple wildcards and increases the size of the vocabulary by the average length of a word in the language in question.\n",
    "2. K-gram: instead of working on entire words (or stems), breaking words into $k-grams$ and using them in the inverted index. This way we can handle multiple wild cards. Despite its flexibility, this solution still has a number of issues: \n",
    "    * the large number of elements in the index: for a $k$-gram the number of possible grams (in English) $(26 + 1) ^ {k}$.Neverthelss the postings lists will increase significanlty\n",
    "    * Not all K-grams are created equally many of them are barely present and many of them are present everywhere: well the same issue araising when using actual words\n",
    "3. Multi-gram: This model introduces the notion of selectivity: the portion of documents that have at least one occurence of the k-gram in question.\n",
    "\n",
    "### Query Correction\n",
    "There are two main different corrections:\n",
    "1. isolated correction\n",
    "2. context correction  \n",
    "in both approaches, building a vocabulary is the starting point. The latter can be either standard or even task domain specific."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lecture 7: Recommender Systems\n",
    "* most current services are based on recommendations and not subscriptions.  \n",
    "* The most notorious approach is to build a vector space. Where certain concepts (either documents, certain words...) so that each item (or piece of text) can be represented as linear representation of such basis. The concept of basis assumes orthogonality, which might not be easily achievable in the context of textual data."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Distributional semantics: Representing a word as a vector in the vector space, the distributional hypothesis states:  \n",
    "the distribution of items is tightly related to its meaning."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lecture 9: Clustering\n",
    "The main idea of improvement is that instead of running our vector representation of the query by $N$ other representations of documents, one can cluster the documents into for example $\\sqrt N$ clusters of size $\\sqrt N$. The first step is to consider the cluster whose centroid is the closest to the representation of the query. This way we have a pretty good space of solutions of size $\\sqrt N$ and the total time complexity will be around $2 \\cdot N$.   \n",
    "There are mainly 2 clustering approachs:\n",
    "1. dividive: the 2nd mainly \n",
    "2. agglomerative\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Among the desirable properties of K-Means:\n",
    "1. The clusters are convex in nature. This property is specially desirable in higher dimension, as the we are sure that the centroid of the point's cluster is indeed the point's closest cluster.\n",
    "2. The clusters are of approximately the same size. The usefulness of such property is task-dependent.  \n",
    "We can say that K-means split the space while other agglomerative algorithms split the data and thus DBSCAN is more efficient at capturing clusters that are more natural to the human eye while K-means will still converge to (a local) optimum that would generally satisfy the 2 conditions mentioned above."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linkage criteria:\n",
    "1. single linkage: smallest distance: DBSCAN\n",
    "2. complete linkage: the largest distance\n",
    "3. energy-inspired approach: taking the variance's growth as the clustering main factor.\n",
    "4. Average distance: KMEANS: centroid and distance-based approaches.\n",
    " "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FAISS THE IDEA: \n",
    "1. the main idea is to use the constant-time retrieval of the inverted index data structre. IIDS expects discrete input: word not vectors. One way to do that is Vector Quantization\n",
    "2. Divide the vector into smaller parts: say sets of dimensions.\n",
    "3. for each set of dimensions (mainly indices within the vectors representations): cluster the obtained dataset and simply assign the ID of the closest centroid to that particular set of dimensions.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Let's elaborate on the 3rd step here a bit. So assuming we have a vector $$V = \\begin{bmatrix}x_1 & x_2 & ... & x_{n} \\end{bmatrix}$$ Assuming we want to group/quantize/summarize every $8$ consecutive dimensions. We apply $\\frac{n}{8}$ clustering operations producing $\\frac{n}{8}$ centroids. Now for each vector $v$, the first $8$ bits will be replaced by the closest centroid in the first $8$ bits clusters, so is the 2nd, 3rd and so on. The query will be represented similarily.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FAISS clusters the documents' representations using **K-means**. Each word is now represented by the centroid of the cluster it belongs to. Furthermore, the resulting word is converting into a discret value: **Vector Quantization**. These values are indexed and saved in an inverted index data structure with is know to have a fetching time of constant complexity.  "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Approximity Graph\n",
    "Random Graph: are graphs generated by some random process built upon a random distribution. They were considered as a model of our social network.\n",
    "Random Graph have 2 characterizin properties:\n",
    "1. small average longest path\n",
    "2. Very low clustering coefficient: A value that numerically represents the nodes' tendancy to form clusters / cliqs. check this [link] (https://en.wikipedia.org/wiki/Clustering_coefficient)   \n",
    "The first is desirable, as we can generally find the nearest neighbor efficiently. The 2nd is indesirable as depending on the starting node, it might take significant time and resources to cover the entire graph.   \n",
    "\n",
    "Modeling the social networks, several types of graphs were used:\n",
    "1. random graphs: the human relationships are chaotic\n",
    "2. (almost) regular graphs: most humans have the same number of friends.\n",
    "3. small world network  \n",
    "The latter can be characterized as follows:\n",
    "* sparse graph\n",
    "* small number of hopes will lead from one node to another: the asymptotic estimation is $Log(N)$ where $N$ is the network's size."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hierarichal Navigable Small World graphs are both fast and accurate. The term \"fast\" here refers to an **expected** logarithmic time complexity. The term **expected** comes with its corner cases where some unlucky users will end up either with very irrelevant results or will have to wait significantly. They only require a distance function between 2 entities. The entities themselves do not need to be vectors.\n",
    "\n",
    "## Search Trees\n",
    "Search Trees were introduced to guarantee the logarithmic time complexity. Let's consider one interesting point. The common characteristic of ***Tree Data structure*** is being stored in a harddrive. Harddrive read operations are costly (in terms of performance). Thus, the Tree data structure is better designed to have a large number of children for each node. Trees guarantee logarithmic time complexity."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## QuadTrees\n",
    "Quadtrees are Search Trees that 4 children nodes associated with each node, generally representing data. Unbalance is an issue. Among the introduced solutions:\n",
    "1. balancing the regions\n",
    "2. Splitting the data in a way that new regions can have at most half of the data in the previous split: the split is no longer split-oriented but data oriented.  \n",
    "\n",
    "Quadtree comes with a number of issues:\n",
    "1. working only on 2-dimentional data\n",
    "2. the memory management can be improved.\n",
    "\n",
    "## K-dTrees\n",
    "K-D trees offers significant enhancements over QuadTree. They can work high dimensional data offering high performance. They require vector representation as well as a predefined metric.  \n",
    "One issue to keep in mind is the following:  \n",
    "1. The K-d tree split each node into 2 subtrees. A dataset of $10^9$ samples can be stored in a K-d Tree of depth $\\approx 30$ meaning, only the first $30$ dimensions out of possibly several hundread dimensions are used in the search leadning to extremely poor results.   \n",
    "\n",
    "Range Trees should be considered as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
      "100  258k  100  258k    0     0   474k      0 --:--:-- --:--:-- --:--:--  474k\n"
     ]
    }
   ],
   "source": [
    "! curl -o hm3.ipynb https://raw.githubusercontent.com/IUCVLab/information-retrieval/main/homeworks/2023/2023S-03%20-%20Index%20data%20structures.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
      "  0  525k    0  2742    0     0   7909      0  0:01:08 --:--:--  0:01:08  7924\n",
      "100  525k  100  525k    0     0   937k      0 --:--:-- --:--:-- --:--:--  937k\n"
     ]
    }
   ],
   "source": [
    "! curl -o world.csv https://raw.githubusercontent.com/IUCVLab/information-retrieval/main/datasets/world.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
      "  1 4749k    1 49152    0     0  75921      0  0:01:04 --:--:--  0:01:04 75969\n",
      "100 4749k  100 4749k    0     0  4203k      0  0:00:01  0:00:01 --:--:-- 4210k\n"
     ]
    }
   ],
   "source": [
    "! curl -o words.txt https://raw.githubusercontent.com/IUCVLab/information-retrieval/main/datasets/words.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('se_env')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "87916a9df29343f518d363a6149a1dfa14832b884faad311882187c1f88054e8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
