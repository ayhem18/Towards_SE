{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview\n",
    "This is my attempt to solve the first assigment in the Information Retrieval course offered in Innopolis University"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests \n",
    "from urllib.parse import quote\n",
    "import hashlib\n",
    "import os\n",
    "\n",
    "class Document:\n",
    "    no_url_msg = \"\\nAN URL MUST BE SET BEFORE PROCEEDING WITH A 'Document' OBJECT\\n\"\n",
    "\n",
    "\n",
    "    def __init__(self, url):\n",
    "        self.url = url\n",
    "\n",
    "    def __get_file_name(self):\n",
    "        if not self.url:\n",
    "            print(self.no_url_msg)\n",
    "            return None\n",
    "        \n",
    "        # first extract the hashed name\n",
    "        file_name = hashlib.md5(self.url.encode('utf-8')).hexdigest()\n",
    "        # save the file in the current directory\n",
    "        file_name = os.path.join(os.getcwd(), f'{file_name}.txt')\n",
    "\n",
    "        return file_name\n",
    "\n",
    "    def get(self):\n",
    "        if not self.load():\n",
    "            if not self.download():\n",
    "                raise FileNotFoundError(self.url)\n",
    "            else:\n",
    "                self.persist()\n",
    "\n",
    "\n",
    "    def download(self):\n",
    "        if not self.url:\n",
    "            print(self.no_url_msg)\n",
    "            return False\n",
    "\n",
    "        r =  requests.get(url=self.url, allow_redirects=True)\n",
    "        if r.status_code != 200:\n",
    "            print(f\"the connection to the site {self.url} \\nwas not successful.\")\n",
    "            return False\n",
    "        \n",
    "        self.content = r.content\n",
    "        return True\n",
    "    \n",
    "\n",
    "    def persist(self):\n",
    "        file_name = self.__get_file_name()\n",
    "        if file_name is None:\n",
    "            return False\n",
    "\n",
    "        try:\n",
    "            with open(file_name, 'wb') as f: \n",
    "                f.write(self.content)\n",
    "            return True\n",
    "\n",
    "        except FileNotFoundError as ffe:\n",
    "            print(\"the file has not been yet created!!\")\n",
    "            return False\n",
    "\n",
    "        except Exception as e :\n",
    "            print(e)\n",
    "            return False\n",
    "\n",
    "\n",
    "    def load(self):\n",
    "        file_name = self.__get_file_name()\n",
    "        if file_name is None:\n",
    "            return False\n",
    "\n",
    "        try:\n",
    "            with open(file_name, 'rb') as f: \n",
    "                # set the file's content to the content field\n",
    "                self.content = f.read()\n",
    "            return True\n",
    "        \n",
    "        except FileNotFoundError as ffe:\n",
    "            # print(\"the file has not been yet created!!\")\n",
    "            return False\n",
    "\n",
    "        except Exception as e :\n",
    "            print(e)\n",
    "            return False\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = Document('http://sprotasov.ru/data/iu.txt')\n",
    "\n",
    "doc.get()\n",
    "assert doc.content, \"Document download failed\"\n",
    "assert \"Code snippets, demos and labs for the course\" in str(doc.content), \"Document content error\"\n",
    "\n",
    "doc.get()\n",
    "assert doc.load(), \"Load should return true for saved document\"\n",
    "assert \"Code snippets, demos and labs for the course\" in str(doc.content), \"Document load from disk error\"\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup as bs\n",
    "from bs4.element import Comment\n",
    "import urllib.parse\n",
    "from urllib.parse import urljoin\n",
    "import re\n",
    "\n",
    "class HtmlDocument(Document):\n",
    "    def __init__(self, url, text_join_str=' '):\n",
    "        super().__init__(url)\n",
    "        # this character is used to join all the pieces of visible text scapped from the site \n",
    "        self.text_join_str = text_join_str\n",
    "\n",
    "    def parse(self):\n",
    "        # this function assumes the self.content is already set\n",
    "        if self.content is None:\n",
    "            print(\"Please make sure to have the 'content' field set\")        \n",
    "            return\n",
    "        # create the soup object to parse the html document\n",
    "        doc_soup = bs(self.content, 'html.parser')\n",
    "        # extract anchors\n",
    "        self.anchors = [(str(link.string).strip() if link.string is not None else '', \n",
    "        urljoin(self.url, link['href'])) for link in doc_soup.find_all('a', href=True)] # filter those anchors with no actual link associated with them\n",
    "        \n",
    "        # extract images\n",
    "        self.images = [urljoin(self.url, img['src']) for img in doc_soup.find_all('img', src=True)] # filter the anchors with no actual image associated with them.\n",
    "        # extract text \n",
    "        # firstly extract all text\n",
    "        raw_text = doc_soup.findAll(string=True)\n",
    "        # secondly filter the text associated with unwanted tags\n",
    "        def plain_text(element):\n",
    "            return  element.parent.name not in ['style', 'script', 'title', 'head', 'meta', '[document]'] and not isinstance(element, Comment) \n",
    "\n",
    "        filtered_text = filter(plain_text, raw_text) # remove unwanted text\n",
    "        # join all the text into a single text by the text_join_char attribute\n",
    "        self.text = re.sub(r\"\\s+\", ' ',self.text_join_str.join([t.strip() for t in filtered_text if t.strip()]))\n",
    "        \n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('telegram', 'https://t.me/sprotasov'), ('email', 'mailto:stanislav.protasov@gmail.com'), ('Curriculum vitae', 'https://docs.google.com/document/d/e/2PACX-1vQqlsxmlbkwp7CypdNg5vcl9zEfE1w6EFppJ2iBbHpZrpOI0AIzFkeu21-Or1_PYlnq1ICyLR1qaNlu/pub'), ('Google Scholar', 'https://scholar.google.ru/citations?user=pDske8oAAAAJ'), ('GitHub', 'https://github.com/str-anger'), ('Track record in Quantum', 'http://sprotasov.ru/q.html'), ('ResearchGate', 'https://www.researchgate.net/profile/Stanislav-Protasov'), ('Публикации в eLibrary', 'http://elibrary.ru/author_items.asp?authorid=789317'), ('Facebook', 'https://www.facebook.com/stanislav.protasov'), ('LinkedIn', 'https://www.linkedin.com/pub/stanislav-protasov/28/651/b38'), ('Research with Stas telegram channel', 'https://t.me/iu_aml'), ('', 'https://t.me/origin_of_species'), ('iTunes', 'https://itunes.apple.com/ru/podcast/происхождение-видов/id1282666034'), ('RSS', 'http://sprotasov.ru/podcast/rss.xml'), ('Automatic testing system', 'http://code-test.ru/'), ('source code', 'https://bitbucket.org/str-anger/stick-rope'), ('Книга \"Давайте объясню: или зачем программисту математика\"', 'http://sprotasov.ru/math_book.html'), ('Материалы на ПостНауке', 'https://postnauka.ru/themes/protasov'), ('Twitter', 'https://twitter.com/07C3')]\n",
      "['https://mc.yandex.ru/watch/53482672', 'http://sprotasov.ru/images/gb.svg', 'http://sprotasov.ru/images/gb.svg', 'http://sprotasov.ru/images/gb.svg', 'http://sprotasov.ru/images/gb.svg', 'http://sprotasov.ru/images/ru.svg', 'http://sprotasov.ru/images/ru.svg', 'http://sprotasov.ru/images/gb.svg', 'http://sprotasov.ru/images/gb.svg', 'http://sprotasov.ru/images/ru.svg', 'http://sprotasov.ru/images/gb.svg', 'http://sprotasov.ru/images/ru.svg', 'http://sprotasov.ru/images/ru.svg', 'http://sprotasov.ru/images/gb.svg']\n",
      "Stanislav Protasov ( telegram , email ) in just few links: Curriculum vitae Google Scholar GitHub Track record in Quantum ResearchGate Публикации в eLibrary Facebook LinkedIn Research with Stas telegram channel Подкаст \"Происхождение видов\": telegram , iTunes , RSS Automatic testing system ( source code ) Книга \"Давайте объясню: или зачем программисту математика\" Материалы на ПостНауке Twitter\n"
     ]
    }
   ],
   "source": [
    "doc = HtmlDocument(\"http://sprotasov.ru\")\n",
    "doc.get()\n",
    "doc.parse()\n",
    "\n",
    "print(doc.anchors)\n",
    "print(doc.images)\n",
    "print(doc.text)\n",
    "\n",
    "\n",
    "assert \"http://sprotasov.ru/images/gb.svg\" in doc.images, \"Error parsing images\"\n",
    "assert any(p[1] == \"https://twitter.com/07C3\" for p in doc.anchors), \"Error parsing links\"\n",
    "assert \"just few links\" in doc.text, \"Error parsing text\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\users\\bouab\\dev\\se_env\\lib\\site-packages (3.8.1)\n",
      "Requirement already satisfied: tqdm in c:\\users\\bouab\\dev\\se_env\\lib\\site-packages (from nltk) (4.64.1)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\bouab\\dev\\se_env\\lib\\site-packages (from nltk) (2022.10.31)\n",
      "Requirement already satisfied: joblib in c:\\users\\bouab\\dev\\se_env\\lib\\site-packages (from nltk) (1.2.0)\n",
      "Requirement already satisfied: click in c:\\users\\bouab\\dev\\se_env\\lib\\site-packages (from nltk) (8.1.3)\n",
      "Requirement already satisfied: colorama in c:\\users\\bouab\\dev\\se_env\\lib\\site-packages (from click->nltk) (0.4.6)\n",
      "Requirement already satisfied: spacy in c:\\users\\bouab\\dev\\se_env\\lib\\site-packages (3.5.0)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\bouab\\dev\\se_env\\lib\\site-packages (from spacy) (1.0.9)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in c:\\users\\bouab\\dev\\se_env\\lib\\site-packages (from spacy) (3.0.12)\n",
      "Requirement already satisfied: thinc<8.2.0,>=8.1.0 in c:\\users\\bouab\\dev\\se_env\\lib\\site-packages (from spacy) (8.1.7)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\bouab\\dev\\se_env\\lib\\site-packages (from spacy) (3.0.8)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\bouab\\dev\\se_env\\lib\\site-packages (from spacy) (3.1.2)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in c:\\users\\bouab\\dev\\se_env\\lib\\site-packages (from spacy) (2.4.5)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in c:\\users\\bouab\\dev\\se_env\\lib\\site-packages (from spacy) (1.1.1)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in c:\\users\\bouab\\dev\\se_env\\lib\\site-packages (from spacy) (2.0.8)\n",
      "Requirement already satisfied: typer<0.8.0,>=0.3.0 in c:\\users\\bouab\\dev\\se_env\\lib\\site-packages (from spacy) (0.7.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\bouab\\dev\\se_env\\lib\\site-packages (from spacy) (4.64.1)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in c:\\users\\bouab\\dev\\se_env\\lib\\site-packages (from spacy) (1.10.4)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in c:\\users\\bouab\\dev\\se_env\\lib\\site-packages (from spacy) (3.3.0)\n",
      "Requirement already satisfied: pathy>=0.10.0 in c:\\users\\bouab\\dev\\se_env\\lib\\site-packages (from spacy) (0.10.1)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\bouab\\dev\\se_env\\lib\\site-packages (from spacy) (23.0)\n",
      "Requirement already satisfied: numpy>=1.15.0 in c:\\users\\bouab\\dev\\se_env\\lib\\site-packages (from spacy) (1.24.2)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\bouab\\dev\\se_env\\lib\\site-packages (from spacy) (2.0.7)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\bouab\\dev\\se_env\\lib\\site-packages (from spacy) (2.28.2)\n",
      "Requirement already satisfied: setuptools in c:\\users\\bouab\\dev\\se_env\\lib\\site-packages (from spacy) (58.1.0)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in c:\\users\\bouab\\dev\\se_env\\lib\\site-packages (from spacy) (6.3.0)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in c:\\users\\bouab\\dev\\se_env\\lib\\site-packages (from spacy) (1.0.4)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in c:\\users\\bouab\\dev\\se_env\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy) (4.4.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\bouab\\dev\\se_env\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.0.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\bouab\\dev\\se_env\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (1.26.14)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\bouab\\dev\\se_env\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\bouab\\dev\\se_env\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2022.12.7)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in c:\\users\\bouab\\dev\\se_env\\lib\\site-packages (from thinc<8.2.0,>=8.1.0->spacy) (0.0.4)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in c:\\users\\bouab\\dev\\se_env\\lib\\site-packages (from thinc<8.2.0,>=8.1.0->spacy) (0.7.9)\n",
      "Requirement already satisfied: colorama in c:\\users\\bouab\\dev\\se_env\\lib\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy) (0.4.6)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in c:\\users\\bouab\\dev\\se_env\\lib\\site-packages (from typer<0.8.0,>=0.3.0->spacy) (8.1.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\bouab\\dev\\se_env\\lib\\site-packages (from jinja2->spacy) (2.1.2)\n",
      "Requirement already satisfied: langdetect in c:\\users\\bouab\\dev\\se_env\\lib\\site-packages (1.0.7)\n",
      "Requirement already satisfied: six in c:\\users\\bouab\\dev\\se_env\\lib\\site-packages (from langdetect) (1.16.0)\n",
      "Requirement already satisfied: langcodes in c:\\users\\bouab\\dev\\se_env\\lib\\site-packages (3.3.0)\n",
      "Requirement already satisfied: language_data in c:\\users\\bouab\\dev\\se_env\\lib\\site-packages (1.1)\n",
      "Requirement already satisfied: marisa-trie<0.8.0,>=0.7.7 in c:\\users\\bouab\\dev\\se_env\\lib\\site-packages (from language_data) (0.7.8)\n",
      "Requirement already satisfied: setuptools in c:\\users\\bouab\\dev\\se_env\\lib\\site-packages (from marisa-trie<0.8.0,>=0.7.7->language_data) (58.1.0)\n"
     ]
    }
   ],
   "source": [
    "! pip install nltk \n",
    "! pip install spacy\n",
    "! pip install langdetect\n",
    "! pip install langcodes\n",
    "! pip install language_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\bouab\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\bouab\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# download the data needed for nltk\n",
    "import nltk\n",
    "nltk.download('punkt') \n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's use some simple NLP tools here\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.tokenize import word_tokenize \n",
    "from langdetect import detect\n",
    "from collections import Counter\n",
    "from langcodes import Language\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from string import punctuation \n",
    "\n",
    "import re\n",
    "\n",
    "punc_regex = r'.*[!\\\"#\\$%&\\'\\(\\)\\*\\+,-\\.:;<=>\\?@\\[\\\\\\]\\^_`{\\|}~«»]+.*'\n",
    "\n",
    "class HtmlDocumentTextData:\n",
    "    \n",
    "    def __init__(self, url):        \n",
    "        self.doc = HtmlDocument(url)\n",
    "        self.doc.get()\n",
    "        self.doc.parse()\n",
    "    \n",
    "    def _detect_language(self):\n",
    "        # to use the power of NLP tools, regardless of the text's language, language detection is needed\n",
    "        text = self.doc.text\n",
    "        # determine the language used in the text\n",
    "        lan_code = detect(text)\n",
    "        # the language variable represents the code of the language and not its standard form\n",
    "        # the langcodes package is in for the rescue\n",
    "        language = Language.make(language=lan_code).display_name().lower()\n",
    "        return language, set(stopwords.words(language))        \n",
    "\n",
    "    def get_sentences(self):\n",
    "        text = self.doc.text\n",
    "        lang, _ = self._detect_language()\n",
    "        # the main limitation of this class is that it assumes the text's language is english\n",
    "        result = sent_tokenize(text, language=lang)\n",
    "        return result\n",
    "    \n",
    "    def get_word_stats(self):\n",
    "        text = self.doc.text\n",
    "        lang, stop_words = self._detect_language()\n",
    "        words = [w.lower().strip() for w in word_tokenize(text, language=lang) if w.lower().strip() not in stop_words and re.match(punc_regex, w.lower().strip()) is None]\n",
    "        # create the counter and map each word to its frequency\n",
    "        counter = Counter()\n",
    "        for w in words:\n",
    "            counter[w] += 1 \n",
    "        return counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('иннополис', 22), ('университет', 12), ('университета', 12), ('центр', 10), ('образование', 8), ('робототехники', 6), ('деятельность', 6), ('управления', 5), ('научные', 5), ('образовательной', 5)]\n"
     ]
    }
   ],
   "source": [
    "doc = HtmlDocumentTextData(\"https://innopolis.university/\")\n",
    "\n",
    "print(doc.get_word_stats().most_common(10))\n",
    "assert [x for x in doc.get_word_stats().most_common(10) if x[0] == 'иннополис'], 'иннополис should be among most common'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# task 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from queue import Queue\n",
    "\n",
    "class Crawler:\n",
    "    def crawler_generator(self, source, depth=1):\n",
    "        if depth <= 1:\n",
    "            html_doc = HtmlDocumentTextData(source)\n",
    "            results =  [html_doc] if html_doc.doc.content else []\n",
    "        else: \n",
    "            top_doc = HtmlDocumentTextData(source)\n",
    "            results = [top_doc] if top_doc.doc.content else []\n",
    "            try:\n",
    "                for link in top_doc.doc.anchors:\n",
    "                    results.extend(self.crawler_generator(link[1], depth=depth-1))\n",
    "            except ValueError:\n",
    "                # the connection to the site was not successful\n",
    "                pass\n",
    "             \n",
    "        return results  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<__main__.HtmlDocumentTextData object at 0x000001C47AAC3D60>, <__main__.HtmlDocumentTextData object at 0x000001C41FA47E80>]\n"
     ]
    }
   ],
   "source": [
    "url = \"http://sprotasov.ru\"\n",
    "\n",
    "craw = Crawler()\n",
    "\n",
    "res = craw.crawler_generator(url, depth=2)\n",
    "\n",
    "print(res)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('se_env')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "87916a9df29343f518d363a6149a1dfa14832b884faad311882187c1f88054e8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
