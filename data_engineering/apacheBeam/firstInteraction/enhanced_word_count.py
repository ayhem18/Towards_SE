"""
This is a simple script to build a data pipeline to count the number of words in a text file excluding stop words.
"""
import re, os

import apache_beam as beam


from pathlib import Path
from typing import Iterable, Union
from apache_beam.io import ReadFromText, WriteToText
from apache_beam.options.pipeline_options import PipelineOptions

# THOUGHT PROCESS 

# 1. read the text file
# 2. split the text into words: this uses FlatMap() because I am converting a single element into multiple elements
# 3. the next step is to remove stop words. I can use some sort of class that inherits from DoFn() and implements the process() method 
# all while using stop words as part of the initialization process. Non-filtered words should be converted to (key, value) pairs 
# 4. group by key word and convert the (key, list) pairs  to  (key, length of list) pairs 
# 5. Write to the output (it is also possible to convert the pair into a specific text format, but that's not particularly important)

# we can define a general class that inherits from DoFn() and implement all the usual NLP processing steps
# such as stemming, lowercasing, removing extra white spaces, etc.




def _clean_temp_files(input_file: Union[str, Path]) -> None:
    # delete any temp file generated by the pipeline 
    input_file_dir = Path(input_file).parent

    files = os.listdir(input_file_dir)

    for f in files:
        if "temp-beam" in f:
            os.remove(os.path.join(input_file_dir, f))


class StopWordsFilter(beam.DoFn):
    def __init__(self, stop_words: Iterable[str]):
        self.stop_words = set([word.lower().strip() for word in stop_words])
    
    def process(self, element):
        if element.lower().strip() not in self.stop_words:
            yield element




def create_pipeline(stop_words: Iterable[str], input_file: Union[str, Path], output_file: Union[str, Path]):
    pip = beam.Pipeline()

    input_file = str(input_file)
    output_file = str(output_file)

    output = (pip 
    | "read" >> ReadFromText(input_file)
    | "split" >> beam.FlatMap(lambda line: re.split(r'[^a-zA-Z\d]+', line))
    | "remove_stop_words" >> beam.ParDo(StopWordsFilter(stop_words)) 
    | "map_to_pairs" >> beam.Map(lambda word: (word.lower().strip(), 1))
    | "group_by_key" >> beam.GroupByKey()
    | "count_words" >> beam.Map(lambda pair: (pair[0], len(pair[1])))
    | "write_output" >> WriteToText(output_file)
    )

    pip.run()

    _clean_temp_files(input_file)

    return output


if __name__ == "__main__":
    script_dir = os.path.dirname(os.path.abspath(__file__))
    stop_words = ["the", "and", "of", "to", "in", "a", "is", "it", "are", "am", "are", "i", "you"]
    input_file = os.path.join(script_dir, "data", "input.txt")
    output_file = os.path.join(script_dir, "data", "output.txt")

    create_pipeline(stop_words, input_file, output_file)

